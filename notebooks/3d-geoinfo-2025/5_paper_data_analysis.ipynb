{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Metrics for Visualization in tikz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "base_folder = Path(\"G:/3D-GeoInfo-2025/data/4_predictions\")\n",
    "output_folder = Path(\"G:/3D-GeoInfo-2025/data/5_evaluation\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "seeds = np.arange(5)\n",
    "epochs = np.arange(20) + 1\n",
    "resolutions = [\"2_5_cm\", \"5_cm\", \"7_5_cm\", \"10_cm\"]\n",
    "experiments = [\"manual_labeling_small\",\n",
    "               \"manual_labeling_ext\",\n",
    "               \"manual_correction_small\",\n",
    "               \"manual_correction_ext\",\n",
    "               \"automatic_labeling_small\",\n",
    "               \"automatic_labeling_ext\"]\n",
    "experiment_abbreviations = {\n",
    "    \"manual_labeling_small\": \"MLS\",\n",
    "    \"manual_labeling_ext\": \"MLE\",\n",
    "    \"manual_correction_small\": \"MCS\",\n",
    "    \"manual_correction_ext\": \"MCE\",\n",
    "    \"automatic_labeling_small\": \"ALS\",\n",
    "    \"automatic_labeling_ext\": \"ALE\",\n",
    "}\n",
    "\n",
    "for resolution in resolutions:\n",
    "    metrics_barchart = []\n",
    "    for idx, experiment in enumerate(experiments):\n",
    "        best_f1_score = -1\n",
    "        best_epoch = -1\n",
    "        best_seed = -1\n",
    "\n",
    "        experiment_metrics = []\n",
    "        metrics_line_chart = []\n",
    "        for epoch in epochs:\n",
    "            metrics = []\n",
    "            for seed in seeds:\n",
    "                current_metrics = []\n",
    "                for subset in [\"train\", \"test\"]:\n",
    "                    subset_name = subset.capitalize()\n",
    "                    metrics_file = base_folder / resolution / experiment / f\"{epoch}_epochs\" / f\"{subset}_metrics_seed_{seed}.csv\"\n",
    "                    metr = pd.read_csv(metrics_file)\n",
    "                    metr = pd.DataFrame([\n",
    "                        metr[\"score\"].to_numpy()],\n",
    "                        columns=[f\"{subset_name}{metric_name.capitalize()}\" for metric_name in metr[\"metric\"].to_list()]\n",
    "                    )\n",
    "                    metr.rename({f\"{subset_name}F1\": f\"{subset_name}FScore\"}, inplace=True, axis=1)\n",
    "                    if subset == \"train\":\n",
    "                        metr[\"Epochs\"] = epoch\n",
    "                        metr[\"Seed\"] = seed\n",
    "\n",
    "                    if subset == \"test\" and metr[\"TestFScore\"].iloc[0] > best_f1_score:\n",
    "                        best_f1_score = metr[\"TestFScore\"].iloc[0]\n",
    "                        best_epoch = epoch\n",
    "                        best_seed = seed\n",
    "                    current_metrics.append(metr)\n",
    "\n",
    "                metrics.append(pd.concat(current_metrics, axis=1))\n",
    "\n",
    "            metrics_boxplot = pd.concat(metrics)\n",
    "            metrics_boxplot.to_csv(output_folder / f\"{resolution}_{experiment}_{epoch}_epochs_boxplot.csv\", index=False)\n",
    "\n",
    "            metr = {\n",
    "                \"Epochs\": epoch,\n",
    "            }\n",
    "            for metric_name in [\"FScore\", \"Precision\", \"Recall\"]:\n",
    "                for subset_name in [\"Train\", \"Test\"]:\n",
    "                    metr[f\"{experiment_abbreviations[experiment]}{subset_name}{metric_name}\"] = metrics_boxplot[f\"{subset_name}{metric_name}\"].to_numpy().mean()\n",
    "                    metr[f\"{experiment_abbreviations[experiment]}{subset_name}{metric_name}Std\"] = metrics_boxplot[f\"{subset_name}{metric_name}\"].to_numpy().std()\n",
    "\n",
    "            metrics_line_chart.append(metr)\n",
    "\n",
    "            if epoch == epochs.max():\n",
    "                metr = {\n",
    "                    \"Index\": idx,\n",
    "                    \"Experiment\": experiment,\n",
    "                    \"Epochs\": epoch,\n",
    "                }\n",
    "\n",
    "                for metric_name in [\"FScore\", \"Precision\", \"Recall\"]:\n",
    "                    for subset_name in [\"Train\", \"Test\"]:\n",
    "                        metr[f\"{subset_name}{metric_name}\"] = metrics_boxplot[f\"{subset_name}{metric_name}\"].to_numpy().mean()\n",
    "                        metr[f\"{subset_name}{metric_name}Std\"] = metrics_boxplot[f\"{subset_name}{metric_name}\"].to_numpy().std()\n",
    "                metrics_barchart.append(metr)\n",
    "\n",
    "        if idx == 0:\n",
    "            metrics_line_chart_df = pd.DataFrame(metrics_line_chart)\n",
    "        else:\n",
    "            metrics_line_chart_df = pd.concat((metrics_line_chart_df, pd.DataFrame(metrics_line_chart)), axis=1)\n",
    "\n",
    "        print(\"----------------\")\n",
    "        print(experiment, resolution)\n",
    "        print(\"best F1-score\", best_f1_score)\n",
    "        print(\"best epoch:\", best_epoch)\n",
    "        print(\"best seed:\", best_seed)\n",
    "        print(\"----------------\")\n",
    "\n",
    "    for subset in [\"test\"]:\n",
    "        subset_name = subset.capitalize()\n",
    "        baseline_metrics_file = base_folder / resolution / \"baseline\" / f\"{subset}_metrics.csv\"\n",
    "        baseline_metrics = pd.read_csv(baseline_metrics_file)\n",
    "        baseline_metrics = pd.DataFrame([\n",
    "            baseline_metrics[\"score\"].to_numpy()],\n",
    "            columns=baseline_metrics[\"metric\"].to_list()\n",
    "        )\n",
    "        metrics_line_chart_df[f\"NoFT{subset_name}FScore\"] = baseline_metrics[\"f1\"].iloc[0]\n",
    "        metrics_line_chart_df[f\"NoFT{subset_name}Precision\"] = baseline_metrics[\"precision\"].iloc[0]\n",
    "        metrics_line_chart_df[f\"NoFT{subset_name}Recall\"] = baseline_metrics[\"recall\"].iloc[0]\n",
    "\n",
    "    metrics_line_chart_df.to_csv(output_folder / f\"linechart_{resolution}.csv\", index=False)\n",
    "\n",
    "    metrics_barchart_df = pd.DataFrame(metrics_barchart)\n",
    "    metrics_barchart_df.to_csv(output_folder / f\"barchart_{resolution}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for resolution in [\"2_5_cm\", \"5_cm\", \"7_5_cm\", \"10_cm\"]:\n",
    "    metrics = pd.read_csv(output_folder / f\"linechart_{resolution}.csv\")\n",
    "    print(\"baseline:\", np.round(metrics[\"NoFTTestFScore\"].unique(), 2))\n",
    "    print(\"metrics\", np.round(metrics[[\"MLSTestFScore\", \"MCSTestFScore\", \"ALSTestFScore\"]].max(), 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for resolution in [\"2_5_cm\", \"5_cm\", \"7_5_cm\", \"10_cm\"]:\n",
    "    metrics = pd.read_csv(output_folder / f\"linechart_{resolution}.csv\")\n",
    "    print(\"metrics\", np.round(metrics[[\"MLETestFScore\", \"MCETestFScore\", \"ALETestFScore\"]].max(), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for resolution in [\"2_5_cm\", \"5_cm\", \"7_5_cm\", \"10_cm\"]:\n",
    "    metrics = pd.read_csv(output_folder / f\"linechart_{resolution}.csv\")\n",
    "    print(\"metrics\", np.round(metrics[[\"MLSTrainFScore\", \"MCSTrainFScore\", \"ALSTrainFScore\"]].max(), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepforest-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
